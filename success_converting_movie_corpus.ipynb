{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "success:converting_movie_corpus.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Code-CloudSG/SpeechRG/blob/Code-CloudSG-patch-1/success_converting_movie_corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOfx5P5EPczi"
      },
      "source": [
        "## Converting the Cornell Movie-Dialogs Corpus into ConvoKit format \n",
        "\n",
        "This notebook is a demonstration of how custom datasets can be converted into Corpus with ConvoKit. \n",
        "\n",
        "The original version of the Cornell Movie-Dialogs Corpus can be downloaded from:  https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html. It contains the following files:\n",
        "\n",
        "* __movie_characters_metadata.txt__ contains information about each movie character\n",
        "* __movie_lines.txt contains__ the actual text of each utterance\n",
        "* __movie_conversations.txt__ contains the structure of the conversations\n",
        "* __movie_titles_metadata.txt__ contains information about each movie title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iUWWh1TNfID",
        "outputId": "74311329-6979-42fa-850d-39344e2ead92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHtKjnmGNqlM",
        "outputId": "108f2d50-e244-4917-9725-4c94d9653ff3"
      },
      "source": [
        "%cd gdrive/My Drive/project_folder\n",
        "!pwd"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/project_folder\n",
            "/content/gdrive/My Drive/project_folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JSr2XTrNk5T",
        "outputId": "fdb6454e-325f-4871-836f-97920825f726"
      },
      "source": [
        "%cd chatbot/Cornell-Conversational-Analysis-Toolkit\n",
        "!pwd\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit\n",
            "/content/gdrive/My Drive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoTfddqsOhQA",
        "outputId": "1ae5d4d9-efc3-4b8a-b3bf-8309f720ff64"
      },
      "source": [
        "%cd examples/dataset-examples"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit/examples/dataset-examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EpFgOwCPEO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8004d1-8745-49d9-defc-25b7d48049d9"
      },
      "source": [
        "%cd /dataset-examples"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/dataset-examples'\n",
            "/content/gdrive/My Drive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit/examples/dataset-examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx2xDSQxWofK",
        "outputId": "0ce722ae-6de1-49a9-924d-f9d15c0feb31"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit/examples/dataset-examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tk15HEFPEZS"
      },
      "source": [
        "from convokit import Corpus, download"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pn_6vlLPrGD",
        "outputId": "74f8ca9b-fccb-48a5-fa38-88e5baf1be62"
      },
      "source": [
        "corpus = Corpus(filename=download(\"movie-corpus\"))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading movie-corpus to /root/.convokit/downloads/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NresLIya_wy",
        "outputId": "41d02ac7-9ce1-4002-f289-2a5512d6fb07"
      },
      "source": [
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-29 10:59:05--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  12.5MB/s    in 0.8s    \n",
            "\n",
            "2021-06-29 10:59:06 (12.5 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAXLhsWnbG0w",
        "outputId": "4d81ad1d-9048-4486-81b9-01817b5355d6"
      },
      "source": [
        "!unzip cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cornell_movie_dialogs_corpus.zip\n",
            "   creating: cornell movie-dialogs corpus/\n",
            "  inflating: cornell movie-dialogs corpus/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/cornell movie-dialogs corpus/\n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
            "  inflating: cornell movie-dialogs corpus/chameleons.pdf  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
            "  inflating: cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_conversations.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_lines.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/raw_script_urls.txt  \n",
            "  inflating: cornell movie-dialogs corpus/README.txt  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._README.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE5AmhowX_DN",
        "outputId": "021c5f1b-ef81-46a3-899e-162ee5ef9594"
      },
      "source": [
        "!wget  http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-29 10:54:58--  http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
            "Resolving zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)... 128.253.51.178\n",
            "Connecting to zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)|128.253.51.178|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip [following]\n",
            "--2021-06-29 10:54:58--  https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
            "Connecting to zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)|128.253.51.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40854701 (39M) [application/zip]\n",
            "Saving to: ‘movie-corpus.zip’\n",
            "\n",
            "movie-corpus.zip    100%[===================>]  38.96M  22.9MB/s    in 1.7s    \n",
            "\n",
            "2021-06-29 10:55:00 (22.9 MB/s) - ‘movie-corpus.zip’ saved [40854701/40854701]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNtyEIxVaLET",
        "outputId": "832eea38-e3d4-4f6e-f7e2-08708afcf261"
      },
      "source": [
        "!unzip movie-corpus.zip"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  movie-corpus.zip\n",
            "   creating: movie-corpus/\n",
            "  inflating: movie-corpus/utterances.jsonl  \n",
            "  inflating: movie-corpus/conversations.json  \n",
            "  inflating: movie-corpus/corpus.json  \n",
            "  inflating: movie-corpus/speakers.json  \n",
            "  inflating: movie-corpus/index.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzHZbeuVX_K9",
        "outputId": "5e757343-95fb-4007-975d-93e564b94ee6"
      },
      "source": [
        "corpus.print_summary_stats()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Speakers: 9035\n",
            "Number of Utterances: 304713\n",
            "Number of Conversations: 83097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8D4zRxoZL30",
        "outputId": "39135a7d-b49b-4b0c-bc4e-a8cb4ce22962"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit/examples/dataset-examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBFGXx9_PyTo",
        "outputId": "fd3130ec-305a-4010-b2e1-86539d7ce250"
      },
      "source": [
        "!pip install convokit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting convokit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/40/c4f9e73856b50487ef8887f9b1356970fc3cafb47c7b0308e58c500de29b/convokit-2.4.5.tar.gz (144kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 30kB 15.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 61kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 102kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 112kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 122kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 133kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 143kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from convokit) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from convokit) (1.1.5)\n",
            "Collecting msgpack-numpy>=0.4.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/19/05/05b8d7c69c6abb36a34325cc3150089bdafc359f0a81fb998d93c5d5c737/msgpack_numpy-0.4.7.1-py2.py3-none-any.whl\n",
            "Collecting spacy>=2.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 236kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from convokit) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from convokit) (0.22.2.post1)\n",
            "Collecting nltk>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.7/dist-packages (from convokit) (0.3.4)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from convokit) (1.0.1)\n",
            "Collecting clean-text>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/f5/f0db7d0185e26f9a85d425fd19d9bd891ff733f2ec0d1ee06791fd7f13b6/clean_text-0.4.0-py3-none-any.whl\n",
            "Collecting unidecode>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->convokit) (2018.9)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (20.9)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/83/1f567d77173dcdf8e57fccd2a9e086d7702f4b42299070506f72d7353d3a/thinc-8.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (618kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 39.3MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.23.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/a5/a727792d000b2a7bfcccbad03b292cd4c2d567d271fc3cab91250c2461e8/spacy_legacy-3.0.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/fa/d43f31874e1f2a9633e4c025be310f2ce7a8350017579e9e837a62630a7e/pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (57.0.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (3.7.4.3)\n",
            "Collecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/ae/ecfa3e2dc267010fa320034be0eb3a8e683dc98dae7e70f92b41605b4d35/pathy-0.6.0-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.0.5)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (4.41.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (0.4.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit) (7.1.2)\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.2MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->convokit) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy>=2.3.5->convokit) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.3.5->convokit) (5.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.1.1->convokit) (0.2.5)\n",
            "Building wheels for collected packages: convokit, ftfy\n",
            "  Building wheel for convokit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-2.4.5-cp37-none-any.whl size=169629 sha256=43e6dc53fe1455bb505cda5992339966e20e3e728ed75b33d56e17d79a800806\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/58/4e/5b040c32554455627c372d6757f727b5520a52430f2e9272ab\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=84fc984d8205150c5d2fa63df5800a778db3de6f33a99d404c09aea543692ce7\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built convokit ftfy\n",
            "Installing collected packages: msgpack-numpy, catalogue, srsly, pydantic, thinc, spacy-legacy, typer, pathy, spacy, nltk, emoji, ftfy, clean-text, unidecode, convokit\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed catalogue-2.0.4 clean-text-0.4.0 convokit-2.4.5 emoji-1.2.0 ftfy-6.0.3 msgpack-numpy-0.4.7.1 nltk-3.6.2 pathy-0.6.0 pydantic-1.7.4 spacy-3.0.6 spacy-legacy-3.0.6 srsly-2.4.1 thinc-8.0.6 typer-0.3.2 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyVOLyYqPczp"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from convokit import Corpus, Speaker, Utterance\n",
        "from collections import defaultdict"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxO4EiHhPczs"
      },
      "source": [
        "### __Constructing the Corpus from a list of Utterances__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd12z2mVPczt"
      },
      "source": [
        "Corpus can be constructed from a list of utterances with:\n",
        "\n",
        "    corpus = Corpus(utterances= custom_utterance_list)\n",
        "    \n",
        "Our goal is to convert the original dataset into this \"custom_utterance_list\", and let ConvoKit will do the rest of the conversion for us. In the context of converting the Movie-Dialogs corpus, we will need the following steps, which will be explained in further detail below:\n",
        "\n",
        "    1. create Speaker objects who are the speakers of the Utterances. Each speaker will correspond to a character in a movie. \n",
        "    2. create the Utterance objects that corresponds to utterances in the movie dialogs  \n",
        "    3. construct the Corpus from the list of Utterance objects \n",
        "    4. incorporate additional information as Conversation/Corpus metadata. \n",
        "\n",
        "We will additionally show how some simple processing can be done. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX2OGozpPczu"
      },
      "source": [
        "### __1. Creating speakers__\n",
        "\n",
        "Each character in a movie is considered a speaker, and there are 9,035 characters in total in this dataset. We will read off metadata for each speaker from __movie_characters_metadata.txt__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBATwzTwPczv"
      },
      "source": [
        "# replace the directory with where your downloaded cornell movie dialogs corpus is saved\n",
        "#data_dir = \"/cornell-movie-dialogs-corpus/\"\n",
        "data_dir = \"/content/gdrive/MyDrive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit/examples/dataset-examples/cornell movie-dialogs corpus/\""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIZk2DkWPczv"
      },
      "source": [
        "with open(data_dir + \"movie_characters_metadata.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "    speaker_data = f.readlines()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxRcteGyPczx"
      },
      "source": [
        "In general, we would directly use the name of the speaker as the name. However, in our case, since only the first name of the movie character is given for most characters, these names may not uniquely map to a character. We will instead use speaker_id provided in the original dataset as speakername, whereas the actual charatcter name will be saved in speaker metadata. Note that this also means we are not able to account for characters that show up in a series of moviews (i.e., characters who share the same name and should actually be regarded as the same character). \n",
        "\n",
        "For this dataset, we include the following information for each speaker:  \n",
        "* name of the character.\n",
        "* idx and name of the movie this charater is from\n",
        "* gender(available for 3,774 characters)\n",
        "* position on movie credits (3,321 characters available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnxMdntzPczy"
      },
      "source": [
        "speaker_meta = {}\n",
        "for speaker in speaker_data:\n",
        "    speaker_info = [info.strip() for info in speaker.split(\"+++$+++\")]\n",
        "    speaker_meta[speaker_info[0]] = {\"character_name\": speaker_info[1],\n",
        "                               \"movie_idx\": speaker_info[2],\n",
        "                               \"movie_name\": speaker_info[3],\n",
        "                               \"gender\": speaker_info[4],\n",
        "                               \"credit_pos\": speaker_info[5]}"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2d4S1KePczz"
      },
      "source": [
        "In general, a Speaker object can be initiated with `speaker(id = <speaker_name>, meta = <speaker_metadata>)`. The following example shows how we create a Speaker object for each unique character in the dataset, which will be used to create Utterances objects later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9zyr5TkPcz0"
      },
      "source": [
        "corpus_speakers = {k: Speaker(id = k, meta = v) for k,v in speaker_meta.items()}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spnZOmsZPcz0"
      },
      "source": [
        "Sanity checking use-level data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0vT2fd2Pcz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f3d7f0-9720-46fa-aa32-c8725d5363ff"
      },
      "source": [
        "print(\"number of speakers in the data = {}\".format(len(corpus_speakers)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of speakers in the data = 9035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQWlPArkPcz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6b0dce-a0f8-4b6d-a6c4-25b3ad2e5a0f"
      },
      "source": [
        "corpus_speakers['u0'].meta"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'character_name': 'BIANCA',\n",
              " 'credit_pos': '4',\n",
              " 'gender': 'f',\n",
              " 'movie_idx': 'm0',\n",
              " 'movie_name': '10 things i hate about you'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElmhIogXPcz3"
      },
      "source": [
        "### __2. Creating utterance objects__\n",
        "Utterances can be found in __movie_lines.txt__. There are 304,713 utterances in total. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi5WinL6Pcz3"
      },
      "source": [
        "with open(data_dir + \"movie_lines.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "    utterance_data = f.readlines()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxbS5xfHPcz4"
      },
      "source": [
        "To instantiate an utterance object, we generally need the following information (all ids should be of type string):\n",
        "- id: representing the unique id of the utterance. \n",
        "- speaker: a ConvoKit speaker object representing the speaker giving the utterance.\n",
        "- root: the id of the root utterance of the conversation.\n",
        "- reply_to: id of the utterance this was a reply to.\n",
        "- timestamp: timestamp of the utterance. \n",
        "- text: text of the utterance.\n",
        "\n",
        "Additional information associated with the utterance may be saved as utterance level metadata. In this case, we consider the movie_id from which this utterance is extracted as an example for metadata. \n",
        "\n",
        "An utterance possessing all the above information may be initiated by `Utterance(id=..., speaker =..., conversation_id =..., reply_to=..., timestamp=..., text =..., meta =...)`. We now create such `Utterance` objects for the utterances in our dataset. Note that normally we would provide `conversation_id` and `reply_to` information at the time of instantiation, but we will defer it to later as such information need to be retrieved from a different file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb0MKKaWPcz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff1c4e5-66fa-4ac7-b247-a7cf6e4a46de"
      },
      "source": [
        "utterance_corpus = {}\n",
        "\n",
        "count = 0\n",
        "for utterance in tqdm(utterance_data):\n",
        "    \n",
        "    utterance_info = [info.strip() for info in utterance.split(\"+++$+++\")]\n",
        "    \n",
        "    if len(utterance_info) < 4:\n",
        "        print(utterance_info)\n",
        "        \n",
        "    try:\n",
        "        idx, speaker, movie_id, text = utterance_info[0], utterance_info[1], utterance_info[2], utterance_info[4]\n",
        "    except:\n",
        "        print(utterance_info)\n",
        "    \n",
        "    meta = {'movie_id': movie_id}\n",
        "    \n",
        "    # root & reply_to will be updated later, timestamp is not applicable \n",
        "    utterance_corpus[idx] = Utterance(id=idx, speaker=corpus_speakers[speaker], text=text, meta=meta)\n",
        "\n",
        "print(\"Total number of utterances = {}\".format(len(utterance_corpus)))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 304713/304713 [00:19<00:00, 15549.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total number of utterances = 304713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbFftxcyPcz6"
      },
      "source": [
        "If we check on the status of an Utterance object, it should now contain an id, the speakers who said them, the actual texts, as well as the movie ids as the metadata: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS-GsidXPcz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3a5af6-a4db-4aa8-cab7-8272ca838007"
      },
      "source": [
        "utterance_corpus['L1044'] "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Utterance({'obj_type': 'utterance', 'meta': {'movie_id': 'm0'}, 'vectors': [], 'speaker': Speaker({'obj_type': 'speaker', 'meta': {'character_name': 'CAMERON', 'movie_idx': 'm0', 'movie_name': '10 things i hate about you', 'gender': 'm', 'credit_pos': '3'}, 'vectors': [], 'owner': None, 'id': 'u2'}), 'conversation_id': None, 'reply_to': None, 'timestamp': None, 'text': 'They do to!', 'owner': None, 'id': 'L1044'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8yfFzVPcz7"
      },
      "source": [
        "#### __Updating root and reply_to information to utterances__\n",
        "__movie_conversations.txt__ provides the structure of conversations that organizes the above utterances. This will allow us to add the missing root and reply_to information to individual utterances. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_edpsi3Pcz8"
      },
      "source": [
        "with open(data_dir + \"movie_conversations.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "    convo_data = f.readlines()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VLyPWJ9Pcz8"
      },
      "source": [
        "import ast"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwxCph8dPcz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f36f823-050c-4d6a-bd1f-75671f08becf"
      },
      "source": [
        "for info in tqdm(convo_data):\n",
        "        \n",
        "    speaker1, speaker2, m, convo = [info.strip() for info in info.split(\"+++$+++\")]\n",
        "\n",
        "    convo_seq = ast.literal_eval(convo)\n",
        "    \n",
        "    # update utterance\n",
        "    conversation_id = convo_seq[0]\n",
        "    \n",
        "    # convo_seq is a list of utterances ids, arranged in conversational order\n",
        "    for i, line in enumerate(convo_seq):\n",
        "        \n",
        "        # sanity checking: speaker giving the utterance is indeed in the pair of characters provided\n",
        "        if utterance_corpus[line].speaker.id not in [speaker1, speaker2]:\n",
        "            print(\"speaker mismatch in line {0}\".format(i))\n",
        "        \n",
        "        utterance_corpus[line].conversation_id = conversation_id\n",
        "        \n",
        "        if i == 0:\n",
        "            utterance_corpus[line].reply_to = None\n",
        "        else:\n",
        "            utterance_corpus[line].reply_to = convo_seq[i-1]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:01<00:00, 58200.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RWdDIDfPcz9"
      },
      "source": [
        "Sanity checking on the status of utterances. After updating root and reply_to information, they should now contain all mandatory fields:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CztQ4PlvPcz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a316174e-b21f-4d7b-c757-22d58b6fffad"
      },
      "source": [
        "utterance_corpus['L666499']"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Utterance({'obj_type': 'utterance', 'meta': {'movie_id': 'm616'}, 'vectors': [], 'speaker': Speaker({'obj_type': 'speaker', 'meta': {'character_name': 'COGHILL', 'movie_idx': 'm616', 'movie_name': 'zulu dawn', 'gender': '?', 'credit_pos': '?'}, 'vectors': [], 'owner': None, 'id': 'u9028'}), 'conversation_id': 'L666497', 'reply_to': 'L666498', 'timestamp': None, 'text': 'How quickly can you move your artillery forward?', 'owner': None, 'id': 'L666499'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZkIgMxePcz-"
      },
      "source": [
        "### __3. Creating corpus from list of utterances__\n",
        "We are now ready to create the movie-corpus. Recall that to instantiate a `Corpus`, we need a list of `Utterance`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS-axa7MPcz-"
      },
      "source": [
        "utterance_list = utterance_corpus.values()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBl3D_6Pcz_"
      },
      "source": [
        "# Note that by default the version number is incremented \n",
        "movie_corpus = Corpus(utterances=utterance_list)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRNKpUYPcz_"
      },
      "source": [
        "ConvoKit will automatically help us create conversations based on the information about the utterances we provide. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nj71r8ePcz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cd4ff2-7684-4528-d218-850c7ee8cf24"
      },
      "source": [
        "print(\"number of conversations in the dataset = {}\".format(len(movie_corpus.get_conversation_ids())))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of conversations in the dataset = 83097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQiUGCQwPc0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9274e39c-79b9-455f-b614-382f9a7cddf6"
      },
      "source": [
        "convo_ids = movie_corpus.get_conversation_ids()\n",
        "for i, convo_idx in enumerate(convo_ids[0:5]):\n",
        "    print(\"sample conversation {}:\".format(i))\n",
        "    print(movie_corpus.get_conversation(convo_idx).get_utterance_ids())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample conversation 0:\n",
            "['L1045', 'L1044']\n",
            "sample conversation 1:\n",
            "['L985', 'L984']\n",
            "sample conversation 2:\n",
            "['L925', 'L924']\n",
            "sample conversation 3:\n",
            "['L872', 'L871', 'L870']\n",
            "sample conversation 4:\n",
            "['L869', 'L868', 'L867', 'L866']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMKG_P9tPc0A"
      },
      "source": [
        "### __4. Updating Conversation and Corpus level metadata__\n",
        "\n",
        "For each `Conversation`, we can add contextual information about the movie, including genres, release year to as `Conversation` metadata. To do that,  we will read off such meta data for each movie from __movie_titles_metadata.txt__, and we will attach them to all `Conversation`s taken from the movie. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Fj6zFBPc0B"
      },
      "source": [
        "with open(data_dir + \"movie_titles_metadata.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "    movie_extra = f.readlines()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZQANh5GPc0B"
      },
      "source": [
        "movie_meta = defaultdict(dict)\n",
        "\n",
        "for movie in movie_extra:\n",
        "    movie_id, title, year, rating, votes, genre  = [info.strip() for info in movie.split(\"+++$+++\")]\n",
        "    movie_meta[movie_id] = {\"movie_name\": title,\n",
        "                            \"release_year\": year,\n",
        "                            \"rating\": rating,\n",
        "                            \"votes\": votes,\n",
        "                            \"genre\": genre}"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSqF5d_sPc0B"
      },
      "source": [
        "For our purpose, the movie_id of a given conversation can be retrieved from the root of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3_LcSZNPc0B"
      },
      "source": [
        "for convo in movie_corpus.iter_conversations():\n",
        "    \n",
        "    # get the movie_id for the conversation by checking from utterance info\n",
        "    convo_id = convo.get_id()\n",
        "    movie_idx = movie_corpus.get_utterance(convo_id).meta['movie_id']\n",
        "    \n",
        "    # add movie idx as meta, and update meta with additional movie information\n",
        "    convo.meta['movie_idx'] = movie_idx\n",
        "    convo.meta.update(movie_meta[movie_idx])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOV31zZzPc0C"
      },
      "source": [
        "If we check the `conversation` metadata, it now includes the above-mentioned fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVjbFciFPc0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ee18de-d6b7-4803-c4b9-c80696694fd8"
      },
      "source": [
        "movie_corpus.get_conversation(\"L609301\").meta"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'genre': \"['action', 'adventure', 'comedy', 'drama', 'war']\",\n",
              " 'movie_idx': 'm570',\n",
              " 'movie_name': 'three kings',\n",
              " 'rating': '7.30',\n",
              " 'release_year': '1999',\n",
              " 'votes': '69757'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4tQOEcNPc0D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAYjDAuEPc0D"
      },
      "source": [
        "We also include the original urls from which these conversations are extracted as corpus metadata. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhk9HlFhPc0D"
      },
      "source": [
        "with open(data_dir + \"raw_script_urls.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "    urls = f.readlines()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEKs7l68Pc0D"
      },
      "source": [
        "movie2url = {}\n",
        "for movie in urls:\n",
        "    movie_id, _, url = [info.strip() for info in movie.split(\"+++$+++\")]\n",
        "    movie2url[movie_id] = url"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wLoE5gdPc0D"
      },
      "source": [
        "movie_corpus.meta['url'] = movie2url"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRcMGLk6Pc0E"
      },
      "source": [
        "Optionally, we can also the original name of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZBhpkddPc0E"
      },
      "source": [
        "movie_corpus.meta['name'] = \"Cornell Movie-Dialogs Corpus\""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVnvUqLjPc0E"
      },
      "source": [
        ""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsvKUSxDPc0E"
      },
      "source": [
        "### __5. Processing utterance texts__\n",
        "\n",
        "We can also \"annotate\" the utterances, e.g., getting dependency parses for them, and save the resultant parses. Here is an example of how this can be done, more examples related to text processing can be found at https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/text-processing/text_preprocessing_demo.ipynb:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPsyQU0UPc0F"
      },
      "source": [
        "from convokit.text_processing import TextParser"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oVuc7xAPc0F"
      },
      "source": [
        "parser = TextParser(verbosity=10000)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P0lU8zsdL0-",
        "outputId": "3898de18-f7d2-445d-ef8a-0823fc6f335c"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-29 11:08:34.964119: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 233kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6dXyFisPc0F",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1444c7d8-2c9c-48d4-bfa8-d299e0f292b9"
      },
      "source": [
        "movie_corpus = parser.transform(movie_corpus)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/304713 utterances processed\n",
            "20000/304713 utterances processed\n",
            "30000/304713 utterances processed\n",
            "40000/304713 utterances processed\n",
            "50000/304713 utterances processed\n",
            "60000/304713 utterances processed\n",
            "70000/304713 utterances processed\n",
            "80000/304713 utterances processed\n",
            "90000/304713 utterances processed\n",
            "100000/304713 utterances processed\n",
            "110000/304713 utterances processed\n",
            "120000/304713 utterances processed\n",
            "130000/304713 utterances processed\n",
            "140000/304713 utterances processed\n",
            "150000/304713 utterances processed\n",
            "160000/304713 utterances processed\n",
            "170000/304713 utterances processed\n",
            "180000/304713 utterances processed\n",
            "190000/304713 utterances processed\n",
            "200000/304713 utterances processed\n",
            "210000/304713 utterances processed\n",
            "220000/304713 utterances processed\n",
            "230000/304713 utterances processed\n",
            "240000/304713 utterances processed\n",
            "250000/304713 utterances processed\n",
            "260000/304713 utterances processed\n",
            "270000/304713 utterances processed\n",
            "280000/304713 utterances processed\n",
            "290000/304713 utterances processed\n",
            "300000/304713 utterances processed\n",
            "304713/304713 utterances processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXkWjflhPc0F"
      },
      "source": [
        "- parses are saved under 'parsed' in utterance meta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdkV9b3kPc0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927a55bd-afde-4afe-9939-446b97536936"
      },
      "source": [
        "movie_corpus.get_utterance('L666499').retrieve_meta('parsed')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rt': 4,\n",
              "  'toks': [{'dep': 'advmod', 'dn': [], 'tag': 'WRB', 'tok': 'How', 'up': 1},\n",
              "   {'dep': 'advmod', 'dn': [0], 'tag': 'RB', 'tok': 'quickly', 'up': 4},\n",
              "   {'dep': 'aux', 'dn': [], 'tag': 'MD', 'tok': 'can', 'up': 4},\n",
              "   {'dep': 'nsubj', 'dn': [], 'tag': 'PRP', 'tok': 'you', 'up': 4},\n",
              "   {'dep': 'ROOT', 'dn': [1, 2, 3, 6, 7, 8], 'tag': 'VB', 'tok': 'move'},\n",
              "   {'dep': 'poss', 'dn': [], 'tag': 'PRP$', 'tok': 'your', 'up': 6},\n",
              "   {'dep': 'dobj', 'dn': [5], 'tag': 'NN', 'tok': 'artillery', 'up': 4},\n",
              "   {'dep': 'advmod', 'dn': [], 'tag': 'RB', 'tok': 'forward', 'up': 4},\n",
              "   {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '?', 'up': 4}]}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0EwrNN_Pc0G"
      },
      "source": [
        "### __Saving created datasets__\n",
        "To complete the final step of dataset conversion, we want to save the dataset such that it can be loaded later for reuse. You may want to specify a name. The default location to find the saved datasets will be __./convokit/saved-copora__ in your home directory, but you can also specify where you want the saved corpora to be. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwAu670jPc0H"
      },
      "source": [
        "# movie_corpus.dump(\"movie-corpus\", base_path = <specify where you prefer to save it to>)\n",
        "# the following would save the Corpus to the default location, i.e., ./convokit/saved-corpora\n",
        "movie_corpus.dump(\"movie-corpus\",  base_path = \"/content/gdrive/MyDrive/project_folder/chatbot/Cornell-Conversational-Analysis-Toolkit/datasets/created/\")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkOwdu3mjZIU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJWstSWbPc0H"
      },
      "source": [
        "After saving, the available info from dataset can be checked directly, without loading. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bumBQKBPc0H"
      },
      "source": [
        "from convokit import meta_index\n",
        "import os.path"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeQbVELdPc0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914485ac-7b4f-4689-885d-e571470a2b33"
      },
      "source": [
        "meta_index(filename = os.path.join(os.path.expanduser(\"~\"), \".convokit/saved-corpora/movie-corpus\"))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversations-index': {'genre': [\"<class 'str'>\"],\n",
              "  'movie_idx': [\"<class 'str'>\"],\n",
              "  'movie_name': [\"<class 'str'>\"],\n",
              "  'rating': [\"<class 'str'>\"],\n",
              "  'release_year': [\"<class 'str'>\"],\n",
              "  'votes': [\"<class 'str'>\"]},\n",
              " 'overall-index': {'name': [\"<class 'str'>\"], 'url': [\"<class 'dict'>\"]},\n",
              " 'speakers-index': {'character_name': [\"<class 'str'>\"],\n",
              "  'credit_pos': [\"<class 'str'>\"],\n",
              "  'gender': [\"<class 'str'>\"],\n",
              "  'movie_idx': [\"<class 'str'>\"],\n",
              "  'movie_name': [\"<class 'str'>\"]},\n",
              " 'utterances-index': {'movie_id': [\"<class 'str'>\"],\n",
              "  'parsed': [\"<class 'list'>\"]},\n",
              " 'vectors': [],\n",
              " 'version': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz2UKkuyPc0I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh1oMOFhPc0I"
      },
      "source": [
        "### __Other ways of conversion__\n",
        "\n",
        "The above method is only one way to convert the dataset. Alternatively, one may follow strictly with the specifications of the expected data format described [here](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/doc/source/data_format.rst) and write out the component files directly. \n",
        "\n",
        "Additional examples of converting datasets originally released in other formats can be found inside the [datasets](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/tree/master/datasets) folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4jwWfwZPc0I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-bFf_-1Pc0I",
        "scrolled": true
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN4jGPyqPc0I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}